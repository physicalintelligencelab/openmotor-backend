{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLMUL1cDJnLkrB7fbdHqdY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/physicalintelligencelab/openmotor-backend/blob/main/Motor_Dataset_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRW9tiF_-_tB",
        "outputId": "20c0bc08-d46a-4bfc-fbff-5e75b69c8dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade requests tqdm\n"
      ],
      "metadata": {
        "id": "C55KTTJF_PyE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ░█▀█░▀█▀░█▀█░█▀▄░█▀█  COLAB OpenMotor SCRAPER 2.3  ░█\n",
        "!pip -q install requests tqdm\n",
        "\n",
        "import csv, json, re, sys, requests, time\n",
        "from urllib.parse import quote_plus\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "KW_POS = [\n",
        "    \"motor learning\", \"motor adaptation\", \"visuomotor\", \"reach adaptation\",\n",
        "    \"sensorimotor adaptation\", \"force field learning\", \"saccade adaptation\",\n",
        "    \"implicit motor\", \"explicit motor\", \"error-based learning\"\n",
        "]\n",
        "KW_NEG = [\"fmri\", \"mri\", \"eeg\", \"meg\"]\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"motor-learning-harvester/2.3 (colab)\"}\n",
        "\n",
        "def good(txt: str) -> bool:\n",
        "    t = txt.lower()\n",
        "    return any(p in t for p in KW_POS) and not any(n in t for n in KW_NEG)\n",
        "\n",
        "def to_html(link_dict, *keys):\n",
        "    for k in keys:\n",
        "        if k in link_dict:\n",
        "            return link_dict[k]\n",
        "    return None\n",
        "\n",
        "def verify(url: str, timeout=2):\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, stream=True,\n",
        "                         allow_redirects=True, timeout=timeout)\n",
        "        size = int(r.headers.get(\"content-length\", 0)) / 1e6 if r.headers.get(\"content-length\") else None\n",
        "        return (r.status_code < 400, size)\n",
        "    except requests.RequestException:\n",
        "        return (False, None)\n",
        "\n",
        "def datacite_hits():\n",
        "    for kw in tqdm(KW_POS, desc=\"DataCite\", ncols=80):\n",
        "        url = (f\"https://api.datacite.org/dois?query={quote_plus(kw)}\"\n",
        "               f\"&resource-type-id=dataset&page[size]=200\")\n",
        "        try:\n",
        "            for item in requests.get(url, headers=HEADERS, timeout=4).json().get(\"data\", []):\n",
        "                attr  = item[\"attributes\"]\n",
        "                title = attr.get(\"title\",\"\")\n",
        "                desc  = attr.get(\"description\",\"\")\n",
        "                if good(title + \" \" + desc) and attr.get(\"url\"):\n",
        "                    yield title, attr[\"url\"], \"DataCite\"\n",
        "        except requests.Timeout:\n",
        "            continue\n",
        "\n",
        "def zenodo_hits():\n",
        "    for kw in tqdm(KW_POS, desc=\"Zenodo\", ncols=80):\n",
        "        url = f\"https://zenodo.org/api/records/?q={quote_plus(kw)}&type=dataset&size=200\"\n",
        "        try:\n",
        "            hits = requests.get(url, headers=HEADERS, timeout=4).json()[\"hits\"][\"hits\"]\n",
        "        except requests.Timeout:\n",
        "            continue\n",
        "        for rec in hits:\n",
        "            meta  = rec[\"metadata\"]\n",
        "            title = meta.get(\"title\",\"\")\n",
        "            desc  = meta.get(\"description\",\"\")\n",
        "            html  = to_html(rec.get(\"links\", {}), \"html\", \"self_html\", \"preview_html\")\n",
        "            if good(title + \" \" + desc) and html and rec.get(\"files\"):\n",
        "                yield title, html, \"Zenodo\"\n",
        "\n",
        "def osf_has_files(node_id):\n",
        "    base = f\"https://api.osf.io/v2/nodes/{node_id}\"\n",
        "    try:\n",
        "        files = requests.get(f\"{base}/files\", headers=HEADERS, timeout=4).json().get(\"data\", [])\n",
        "        if files: return True\n",
        "\n",
        "        kids = requests.get(f\"{base}/children\", headers=HEADERS, timeout=4).json().get(\"data\", [])\n",
        "        for kid in kids:\n",
        "            if requests.get(kid[\"relationships\"][\"files\"][\"links\"][\"related\"][\"href\"],\n",
        "                            headers=HEADERS, timeout=4).json().get(\"data\", []):\n",
        "                return True\n",
        "    except requests.Timeout:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "def osf_hits():\n",
        "    for kw in tqdm(KW_POS, desc=\"OSF\", ncols=80):\n",
        "        url = (f\"https://api.osf.io/v2/search/?q={quote_plus(kw)}\"\n",
        "               f\"&filter[resource_type]=project&per_page=100\")\n",
        "        try:\n",
        "            data = requests.get(url, headers=HEADERS, timeout=4).json().get(\"data\", [])\n",
        "        except requests.Timeout:\n",
        "            continue\n",
        "        for item in data:\n",
        "            if not item or \"attributes\" not in item: continue\n",
        "            attrs = item[\"attributes\"]; title = attrs.get(\"title\",\"\")\n",
        "            if not good(title): continue\n",
        "            link  = attrs.get(\"public_url\") or to_html(item.get(\"links\", {}), \"html\")\n",
        "            if not link: continue\n",
        "            node_id = item[\"id\"]\n",
        "            if osf_has_files(node_id):\n",
        "                yield title, link, \"OSF\"\n",
        "\n",
        "_pat = re.compile(r'\"/datasets/(ds\\d{6,})\"')\n",
        "def openneuro_hits():\n",
        "    for kw in tqdm(KW_POS, desc=\"OpenNeuro\", ncols=80):\n",
        "        try:\n",
        "            html = requests.get(f\"https://openneuro.org/search?q={quote_plus(kw)}\",\n",
        "                                headers=HEADERS, timeout=4).text\n",
        "            for ds in set(_pat.findall(html)):\n",
        "                yield f\"OpenNeuro {ds}\", f\"https://openneuro.org/datasets/{ds}\", \"OpenNeuro\"\n",
        "        except requests.Timeout:\n",
        "            continue\n",
        "\n",
        "def harvest():\n",
        "    gens = [datacite_hits, zenodo_hits, osf_hits, openneuro_hits]\n",
        "    print(\"\\n🔎  Collecting candidates …\")\n",
        "    cand = []\n",
        "    for g in gens: cand.extend(list(g()))\n",
        "    print(f\"\\n⚙️   {len(cand)} raw candidates. Verifying …\")\n",
        "\n",
        "    ok_rows, seen = [], set()\n",
        "    with ThreadPoolExecutor(max_workers=20) as pool:\n",
        "        for (title, url, src), (ok, size) in tqdm(\n",
        "            zip(cand, pool.map(lambda x: verify(x[1]), cand)),\n",
        "            total=len(cand), ncols=80, desc=\"Verify\"):\n",
        "            if ok and url not in seen:\n",
        "                seen.add(url)\n",
        "                ok_rows.append((title, url, src, size if size else \"\"))\n",
        "    return ok_rows\n",
        "\n",
        "def main(out=\"/content/motor_learning_datasets.csv\"):\n",
        "    rows = harvest()\n",
        "    if not rows:\n",
        "        sys.exit(\"❌  Still no live motor-learning datasets; APIs may be blocked.\")\n",
        "    with open(out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerows([(\"title\",\"url\",\"source\",\"size_MB\"), *rows])\n",
        "    print(f\"\\n Saved {len(rows)} datasets → {out}\")\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMjQmCY4_VIQ",
        "outputId": "0ade8599-ab28-42c6-b454-4afb4cf98be0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎  Collecting candidates …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DataCite: 100%|█████████████████████████████████| 10/10 [00:28<00:00,  2.83s/it]\n",
            "Zenodo: 100%|███████████████████████████████████| 10/10 [00:42<00:00,  4.29s/it]\n",
            "OSF: 100%|██████████████████████████████████████| 10/10 [00:41<00:00,  4.12s/it]\n",
            "OpenNeuro: 100%|████████████████████████████████| 10/10 [00:00<00:00, 64.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚙️   62 raw candidates. Verifying …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verify: 100%|███████████████████████████████████| 62/62 [00:04<00:00, 14.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅  Saved 56 datasets → /content/motor_learning_datasets.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}